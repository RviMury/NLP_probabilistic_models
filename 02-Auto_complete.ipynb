{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"./Data/en_US.twitter.txt\", \"r\",encoding=\"utf8\") as f:\n",
    "    tw_data = f.read()\n",
    "print(\"Data type:\", type(tw_data))\n",
    "print(\"Number of letters:\", len(tw_data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(tw_data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(tw_data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data: str):\n",
    "    '''The function removes the /n from the text file and tokenixes the sentences\n",
    "    Returns : list of lists (eg: [['I', 'Love','cats'], ['I', 'hate', 'dogs']])\n",
    "    '''\n",
    "    sentences=data.split('/n')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]  ## ['I Love cats',I hate'dogs']]\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences: \n",
    "        sentence = sentence.lower()\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 3, 'Love': 2, 'cats': 1, 'hate': 1, 'dogs': 1, 'this': 1, 'song': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(tokenized_sentences: list):\n",
    "    '''\n",
    "    tokenized_sentences: a list of lists with token of each sentences\n",
    "    returns: dict for count of each word\n",
    "    '''\n",
    "    word_counts={}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for word in sentence:\n",
    "            word_counts[word]=word_counts.get(word, 0)+1\n",
    "    return word_counts\n",
    "\n",
    "## Test the above function\n",
    "test_data=[['I', 'Love','cats'], ['I', 'hate', 'dogs'],['I', 'Love', 'this', 'song']]\n",
    "count_words(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'Love', 'cats', 'this', 'song']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_vocab(tokenized_sentences, min_freq=2):\n",
    "    '''\n",
    "    tokenized_sentences: a list of lists with token of each sentences\n",
    "    min_freq: (default=2) In vocabulary oly those words are to be kept whose freq of occurance in equal of greater than min_freq\n",
    "\n",
    "    returns: A list , Our vocabulary\n",
    "    '''\n",
    "    vocab=[]\n",
    "    word_counts=count_words(tokenized_sentences)\n",
    "    for word, count in word_counts.items():\n",
    "        if count>=min_freq:\n",
    "            vocab.append(word)\n",
    "    return vocab\n",
    "\n",
    "## Test the above function\n",
    "test_data=[['i', 'Love','cats'], ['cats', 'hate', 'dogs'],['i', 'Love', 'this', 'song'],\n",
    "           ['this','song', 'is','my','sould' ]]\n",
    "create_vocab(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'Love', 'cats', 'this', 'song']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['i', 'Love', 'cats'],\n",
       " ['cats', '<unk>', '<unk>'],\n",
       " ['i', 'Love', 'this', 'song'],\n",
       " ['this', 'song', '<unk>', '<unk>', '<unk>']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_out_of_vocab_words(tokenized_sentences: list, vocabulary:list,unknown_token='<unk>'):\n",
    "    '''\n",
    "    it removes the out of vocabulary words from the tokenized sentences\n",
    "\n",
    "    Returns: A list of list i.e tokenized corpus from which out of vocabulary words sentences are removed\n",
    "    '''\n",
    "    vocabulary=set(vocabulary) ### for speed\n",
    "    replaced_tokenized_sentences=[]\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        replace_sentences=[]\n",
    "\n",
    "        for token in sentence:\n",
    "            if token in vocabulary:\n",
    "                replace_sentences.append(token)\n",
    "            else:\n",
    "                replace_sentences.append(unknown_token)\n",
    "        replaced_tokenized_sentences.append(replace_sentences)\n",
    "\n",
    "    return replaced_tokenized_sentences\n",
    "\n",
    "## Testing Above Function\n",
    "test_token_scentence=[['i', 'Love','cats'], ['cats', 'hate', 'dogs'],['i', 'Love', 'this', 'song'],\n",
    "                      ['this','song', 'is','my','sould' ]]\n",
    "vocab=create_vocab(test_token_scentence)\n",
    "print(vocab)\n",
    "remove_out_of_vocab_words(test_token_scentence,vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', '<s>'): 4,\n",
       " ('<s>', 'they'): 1,\n",
       " ('they', 'all'): 1,\n",
       " ('all', 'hate'): 1,\n",
       " ('hate', 'cats'): 1,\n",
       " ('cats', '<e>'): 1,\n",
       " ('<s>', 'cats'): 2,\n",
       " ('cats', 'hate'): 2,\n",
       " ('hate', 'dogs'): 1,\n",
       " ('dogs', '<e>'): 1,\n",
       " ('<s>', 'i'): 1,\n",
       " ('i', 'Love'): 1,\n",
       " ('Love', 'this'): 1,\n",
       " ('this', 'song'): 1,\n",
       " ('song', '<e>'): 1,\n",
       " ('hate', 'everything'): 1,\n",
       " ('everything', '<e>'): 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_n_counts(data: list,n: int, start_token='<s>', end_token='<e>'):\n",
    "    '''\n",
    "    Following function create a dictionnary for n-gram\n",
    "\n",
    "    data: Tonkenized sentences from which unknown words are removed\n",
    "    n: N in N-gram\n",
    "    Start_token:\n",
    "    end_token:\n",
    "\n",
    "    Returns : A dictionary of n-gram counts\n",
    "    '''\n",
    "    n_grams_count={}\n",
    "    for sentence in data:\n",
    "        sentence=[start_token]*n+sentence+[end_token]\n",
    "        sentence=tuple(sentence)\n",
    "\n",
    "        for i in range(0, len(sentence)-n+1):\n",
    "            new_gram=sentence[i:i+n]\n",
    "            n_grams_count[new_gram]=n_grams_count.get(new_gram,0)+1\n",
    "    return n_grams_count\n",
    "\n",
    "## Test function\n",
    "test_token=[['they','all','hate','cats'], ['cats', 'hate', 'dogs'],['i', 'Love', 'this', 'song'],\n",
    "             ['cats','hate','everything' ]]\n",
    "\n",
    "create_n_counts(test_token, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_probability(word:str, previous_n_gram:list, n_gram_counts:dict, n_plus1_counts:dict, vocab_size:int, smoothing_parameter=1):\n",
    "    '''\n",
    "    Given a word and a previous n-gram, it computes the probability that given word comes after n-gram\n",
    "    Returns: int\n",
    "    '''\n",
    "    n_plus1_gram=previous_n_gram+[word]\n",
    "    n_plus1_gram=tuple(n_plus1_gram)\n",
    "    previous_n_gram=tuple(previous_n_gram)\n",
    "    \n",
    "    previous_n_gram_count=n_gram_counts.get(previous_n_gram,0)\n",
    "    denominator=previous_n_gram_count+(vocab_size*smoothing_parameter)\n",
    "\n",
    "    current_n_gram_count=n_plus1_counts.get(n_plus1_gram,0)\n",
    "    numerator=current_n_gram_count+smoothing_parameter\n",
    "    prob=numerator/denominator\n",
    "    return prob\n",
    "\n",
    "### TEST ABOVE FUNCTION\n",
    "test_token=[['they','all','hate','cats'], ['cats', 'hate', 'dogs'],['i', 'Love', 'this', 'song'],\n",
    "             ['cats','hate','everything' ]]\n",
    "unigram=create_n_counts(test_token, 1)\n",
    "bigram=create_n_counts(test_token, 2)\n",
    "vocab_size=len(create_vocab(test_token, min_freq=2))\n",
    "estimate_probability('hate',['hate'],unigram, bigram,vocab_size) ## OUTPUT SHOULD BE 0.2 if k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hate': 0.14285714285714285,\n",
       " 'cats': 0.2857142857142857,\n",
       " '<s>': 0.14285714285714285,\n",
       " '<e>': 0.14285714285714285}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_all_probabilites(previous_n_gram: list,n_gram_counts:dict, n_plus1_gram_counts:dict, vocabulary:list,start_token='<s>', end_token='<e>',smoothing_parameter=1):\n",
    "    '''\n",
    "    The function estimate_probability estimates the probability for a given word and a previous_n_gram, this fuction calculates the \n",
    "    probability of all words in our vocabulary for a given previous_n_gram\n",
    "    '''\n",
    "    vocabulary=vocabulary+[start_token]+[end_token]\n",
    "    vocabulary=tuple(vocabulary)\n",
    "    vocab_size=len(vocabulary)\n",
    "    probability_dict={}\n",
    "\n",
    "    for word in vocabulary:\n",
    "        probability_dict[word]=estimate_probability(word, previous_n_gram,n_gram_counts,n_plus1_gram_counts,vocab_size,smoothing_parameter=smoothing_parameter)\n",
    "    \n",
    "    return probability_dict\n",
    "\n",
    "## Test above function\n",
    "test_token=[['they','all','hate','cats'], ['cats', 'hate', 'dogs'],['i', 'Love', 'this', 'song'],\n",
    "             ['cats','hate','everything' ]]\n",
    "unigram=create_n_counts(test_token, 1)\n",
    "bigram=create_n_counts(test_token, 2)\n",
    "vocab=create_vocab(test_token, min_freq=2)\n",
    "estimate_all_probabilites(['hate'],unigram, bigram,vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggest a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_token,n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', start_token='<s>',smoothing_parameter=1,start_with=None):\n",
    "    '''\n",
    "    starts_with is an parameter that you could specify that your next word starts with said letter\n",
    "    Returns : a tuple (word, probability)\n",
    "    '''\n",
    "    n=len(list(n_gram_counts.keys())[0]) ## previous token could be larger than n of N_gram, thus finding the value of N\n",
    "    previous_token=previous_token[-n:]\n",
    "\n",
    "    probabilities=estimate_all_probabilites(previous_token, n_gram_counts, n_plus1_gram_counts,vocabulary,start_token=start_token,\n",
    "                                            end_token=end_token,smoothing_parameter=smoothing_parameter)\n",
    "    suggested_word=None\n",
    "    max_prob=0\n",
    "\n",
    "    for word, prob in probabilities.items():\n",
    "        if start_with!=None:\n",
    "            if not word.startswith(start_with):\n",
    "                continue\n",
    "        if prob>max_prob:\n",
    "            max_prob=prob\n",
    "            suggested_word=word\n",
    "    return suggested_word,max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cats', 0.2857142857142857)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token=[['they','all','hate','cats'], ['cats', 'hate', 'dogs'],['i', 'Love', 'this', 'song'],\n",
    "             ['cats','hate','everything' ]]\n",
    "unigram=create_n_counts(test_token, 1)\n",
    "bigram=create_n_counts(test_token, 2)\n",
    "vocab=create_vocab(test_token, min_freq=2)\n",
    "suggest_a_word(['hate'],unigram, bigram,vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now create n_grams from twitter data set and output words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word using unigram: it , with probability of 0.008687853819853148\n",
      "Next word using bigram: a , with probability of 0.002569043031470777 \n",
      "Next word using trigram: a , with probability of 0.000354588972282962 \n"
     ]
    }
   ],
   "source": [
    "## preprocessing twitter data\n",
    "tokenized_tw_data=tokenize_data(tw_data)\n",
    "tw_vocab=create_vocab(tokenized_tw_data, min_freq=2)\n",
    "tw_processed_data=remove_out_of_vocab_words(tokenized_tw_data,tw_vocab)\n",
    "#######################################\n",
    "\n",
    "## Creating N-Grams from twitter data ##\n",
    "tw_unigram=create_n_counts(tw_processed_data,1)\n",
    "tw_bigram=create_n_counts(tw_processed_data,2)\n",
    "tw_trigram=create_n_counts(tw_processed_data,3)\n",
    "tw_quadgram=create_n_counts(tw_processed_data,4)\n",
    "#########################################\n",
    "previous_token=['how','to','make']\n",
    "\n",
    "unigram_suggestion=suggest_a_word(previous_token,tw_unigram, tw_bigram, tw_vocab)\n",
    "bigram_suggestion=suggest_a_word(previous_token,tw_bigram, tw_trigram, tw_vocab)\n",
    "trigram_suggestion=suggest_a_word(previous_token, tw_trigram, tw_quadgram,tw_vocab)\n",
    "\n",
    "print(f'Next word using unigram: {unigram_suggestion[0]} , with probability of {unigram_suggestion[1]}')\n",
    "print(f'Next word using bigram: {bigram_suggestion[0]} , with probability of {bigram_suggestion[1]} ',)\n",
    "print(f'Next word using trigram: {trigram_suggestion[0]} , with probability of {trigram_suggestion[1]} ')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting New trings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_token=[['dogs', 'love', 'this','song'],['i', 'Love', 'this', 'song','and'],['that','song','makes', 'my','day'],\n",
    "             ['cats','hate','everything' ],['this','song', 'is', 'crazy'],['this','song','makes','me', 'dance'],['this','song','makes','me','emotional']]\n",
    "\n",
    "unigram=create_n_counts(test_token, 1)\n",
    "bigram=create_n_counts(test_token, 2)\n",
    "trigram=create_n_counts(test_token, 3)\n",
    "quadgram=create_n_counts(test_token, 4)\n",
    "fivgram=create_n_counts(test_token, 5)\n",
    "hexagram=create_n_counts(test_token, 6)\n",
    "mapping={1:unigram,\n",
    "2:bigram,\n",
    "3:trigram,\n",
    "4:quadgram,\n",
    "5:fivgram,\n",
    "6:hexagram\n",
    "}\n",
    "\n",
    "vocab=create_vocab(test_token, min_freq=2)\n",
    "word=None\n",
    "word_list=[]\n",
    "prob_list=[]\n",
    "previous_token=['cold']\n",
    "count=0\n",
    "while word!='<e>' and count<10:\n",
    "    if len(previous_token)==6:\n",
    "        previous_token=previous_token[-5:]\n",
    "    initial_gram=mapping[len(previous_token)]\n",
    "    nex_gram=mapping[len(previous_token)+1]\n",
    "    word,prob=suggest_a_word(previous_token, bigram,trigram,vocab)\n",
    "    word_list.append(word)\n",
    "    prob_list.append(prob)\n",
    "    if previous_token[-1]==word:\n",
    "        break\n",
    "    previous_token+=[word]\n",
    "    print(previous_token)\n",
    "    count+=1\n",
    "    #print(word)\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
