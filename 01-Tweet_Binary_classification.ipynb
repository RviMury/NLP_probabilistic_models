{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following Code using logestic regression to predict (positive or negative) the mood of the scentence. The model is trained on NLTK's internal twitter data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples# sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads sample twitter dataset.\n",
    "#nltk.download('twitter_samples')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n"
     ]
    }
   ],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Seperating Train and test\n",
    "train_pos,train_neg =all_positive_tweets[:4000],all_negative_tweets[:4000]\n",
    "test_pos, test_neg=all_positive_tweets[4000:],all_negative_tweets[4000:]\n",
    "\n",
    "y_train_pos,y_train_neg=[1]*len(train_pos),[0]*len(train_neg)\n",
    "y_test_pos,y_test_neg=[1]*len(test_pos),[0]*len(test_neg)\n",
    "\n",
    "## Train and Test Set\n",
    "train_sen_X=train_pos+train_neg\n",
    "train_sen_y=y_train_pos+y_train_neg\n",
    "#random.Random(4).shuffle(train_sen_X)\n",
    "#random.Random(4).shuffle(train_sen_y)\n",
    "\n",
    "test_sen_X=test_pos+test_neg\n",
    "test_sen_y=y_test_pos+y_test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_tweets(scentences_list,stopwords=stopwords):\n",
    "    '''Follwing operations are performed on the list of tweets fed into the function\n",
    "    1. handles, hyperlinks and hashtags are removed\n",
    "    2. Tokenization of words is done\n",
    "    3. Stopwords and puncutation are removed\n",
    "\n",
    "    OUTPUT: \n",
    "    '''\n",
    "    stopwords_english = stopwords.words('english') \n",
    "    output=[]\n",
    "    for scentence in scentences_list:\n",
    "        scentence = re.sub(r'^RT[\\s]+', '', scentence)\n",
    "        scentence = re.sub(r'https?://[^\\s\\n\\r]+', '', scentence)\n",
    "        scentence = re.sub(r'#', '', scentence)\n",
    "        scentence = re.sub(r'#', '', scentence)\n",
    "\n",
    "        ## Creating a list of words in a particular scentence, making all small case\n",
    "        tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "        token_scentence = tokenizer.tokenize(scentence)\n",
    "\n",
    "        ## removing punctuation and stop words\n",
    "        scentence_clean = []\n",
    "        for word in token_scentence: # Go through every word in your tokens list\n",
    "            if (word not in stopwords_english and word not in string.punctuation): # remove stopwords:  # remove punctuation\n",
    "                scentence_clean.append(word)\n",
    "\n",
    "        ## Stemming: (leran, learning, learned)\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_scentence=[]\n",
    "        for word in scentence_clean:\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            stemmed_scentence.append(stem_word)  # append to the list\n",
    "\n",
    "        output.append(stemmed_scentence)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating freq dictionary\n",
    "def creating_freq_dict(preprocessed_scentences,y_tags=[1,1,1,1,1]):\n",
    "    '''\n",
    "    Preprocessed_scentemces are the list of list\n",
    "    Output: dictionary for (word,y) as key and value as freq.\n",
    "    '''\n",
    "    di={}\n",
    "    for i in range(len(preprocessed_scentences)):\n",
    "        for word in preprocessed_scentences[i]:\n",
    "            di[(word,y_tags[i])]=di.get((word,y_tags[i]),0)+1\n",
    "    return di\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorization of Cleaned Scentences\n",
    "def vectorize_cleaned_scentences(freq_dict,preprocessed_scen ):\n",
    "    '''Preprocessed_sen is output from preprocessing_tweets which is a list of list of words\n",
    "    freq_dict: dictionary of freq'''\n",
    "    \n",
    "    X=np.zeros(shape=(len(preprocessed_scen),3))\n",
    "    for i in range(len(preprocessed_scen)):\n",
    "        for word in preprocessed_scen[i]:\n",
    "            pos_freq=freq_dict.get((word,1),0)\n",
    "            neg_freq=freq_dict.get((word,0),0)\n",
    "        X[i]=np.array([1,pos_freq,neg_freq])\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_logestic(train_sentences_X:list,train_sentences_tags:list,test_sentences_X:list, test_sentences_tags:list):\n",
    "    '''\n",
    "    train_sentences_X: List of sentences used in training\n",
    "    train_sentences_tags: tags (pos or neg) for training sentences\n",
    "\n",
    "    Output: Trained model, train_accurancy,test accuracy\n",
    "    '''\n",
    "    model=LogisticRegression(max_iter=1000)\n",
    "    ## Preprocessing Tweets ##\n",
    "    X_train=preprocessing_tweets(train_sentences_X)\n",
    "    X_test=preprocessing_tweets(test_sentences_X)\n",
    "    y_train=np.array(train_sentences_tags)\n",
    "    y_test=np.array(test_sentences_tags)\n",
    "    ##################################\n",
    "\n",
    "    ## Vectorizing Train test sentences ####\n",
    "    freqs_dict=creating_freq_dict(X_train,y_train)\n",
    "    X_train=vectorize_cleaned_scentences(freqs_dict,X_train)\n",
    "    X_test=vectorize_cleaned_scentences(freqs_dict,X_test)\n",
    "    ###########################################\n",
    "\n",
    "    ## Training ###\n",
    "    model.fit(X_train,y_train)\n",
    "    predictions_test=model.predict(X_test)\n",
    "    predictions_train=model.predict(X_train)\n",
    "\n",
    "    accuracy_train=np.sum(y_train==predictions_train)/len(y_train)\n",
    "    accuracy_test=np.sum(y_test==predictions_test)/len(y_test)\n",
    "    \n",
    "\n",
    "    return model,accuracy_train,accuracy_test,freqs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy is 0.896875\n",
      "Test Accuracy is 0.882\n"
     ]
    }
   ],
   "source": [
    "model, train_acc, test_acc,frequency_dict=training_logestic(train_sen_X,train_sen_y,test_sen_X,test_sen_y)\n",
    "print(f'Training Accuracy is {train_acc}')\n",
    "print(f'Test Accuracy is {test_acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test your own scentences\n",
    "In order to misclassify questionable scentences we have created a third class, Questionable, to prevent misclassify questionable scentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_custom_scentences(model=model,frequency_dict=frequency_dict):\n",
    "\n",
    "    interpret_dict={1:'Positive',0:'Negative',0.5: 'Questionable'}\n",
    "    color_dict={1:'\\033[92m ', 0:'\\033[91m ',0.5: '\\033[94m'}\n",
    "    bold='\\033[1m'\n",
    "\n",
    "    scentence=input('Enter Your Scentence:')\n",
    "    print(bold+'Input Scentence: ',scentence)\n",
    "    scentence=[scentence]\n",
    "    X=preprocessing_tweets(scentence)\n",
    "    X=vectorize_cleaned_scentences(frequency_dict,X)\n",
    "    \n",
    "    #result=model.predict(X)\n",
    "    #print(result)\n",
    "    \n",
    "    proba= model.predict_proba(X)[0][0]\n",
    "    if proba>0.6:\n",
    "        result=0\n",
    "    elif proba<0.4:\n",
    "        result=1\n",
    "    else:\n",
    "        result=0.5\n",
    "    \n",
    "    result_statement=interpret_dict[result]\n",
    "    print(f'{bold+color_dict[result]}The Scentence is predicted {result_statement}')\n",
    "    print('Probability of prediction being negative is:', proba)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput Scentence:  how was your day\n",
      "\u001b[1m\u001b[92m The Scentence is predicted Positive\n",
      "Probability of prediction being negative is: 0.3197354964014757\n"
     ]
    }
   ],
   "source": [
    "testing_custom_scentences()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ceavats to the model\n",
    "-  The model is basically trained on frequency of word occurances, It doesn't take into account the words preceeding and trailing words.\n",
    "-  Model is trained on tweets and tweets have a little different fornat than actual coversations,thus results my differ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c838a6f083e17265e8094c63df2ff2c393cf8bb13e8e9c87afb0ba984ea81733"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
